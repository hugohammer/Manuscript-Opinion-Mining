\documentclass[11pt]{article}

\usepackage{acl2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}

\usepackage{graphicx}
\usepackage{fixltx2e,amsmath}
\usepackage{multirow}

\MakeRobust{\eqref}

\bibliographystyle{acl}

\linespread{1.0}

\begin{document}

\begin{center}
  \textbf{\Large{Classifying opinions in online political discussion -- comparison of two methods}}

\vspace{5mm}

\end{center}

\begin{abstract}
Bla bla
\end{abstract}

keywords: \textit{}

\section{Introduction}
\label{sec:introduction}

Over the past years there has been an alarming growth in hate against minorities like Muslims, Jews, Gypsies and gays in Europe, driven by right wing populism parties and extremist organizations \cite{r4,r11}. A similar increase in hate speech has been observed on the Internet \cite{r6,s2}, and experts are concerned that individuals influenced by this web content may resort to violence as a result \cite{Strommen12,Sunde13}. Hateful speech is not only observed on extremst sites, but also as comments on e.g. Twitter, YouTube and online newspaper articles.  

Social media and online discussions contain a wealth of information which can make us able to understand the extent of hate speech on the Internet. However, it turns out that academia is lacking research on social media and online radicalization \cite{s1}. Opinion mining is the discipline of automatically extracting opinions from a text material and may be one important tool in the understanding online radicalization. Opinion mining has mostly been used to analyze opinions in comments and reviews about commercial products, but there are also examples of opinion mining towards political tweets and discussions, see e.g. Tumasjan et al. \shortcite{Tumasjan2010}; Chen et al. \shortcite{Chen10}. Opinion mining towards political discussions is known to be hard since citations, irony and sarcasm is very common \cite{Bing12}.

Opinion classification is perhaps the most studied topic within opinion mining. It aims to classify a set of text as either positive or negative and sometimes also neutral. There are mainly two approaches, one based on machine learning and one based on based on using a list of words with given sentiment scores (lexical approach). One simple lexical approach is to count the number of words with positive and negative sentiment in the document as suggested by Hu and Liu \shortcite{Hu04}. One may classify the opinion of larger documents like movie or product reviews or smaller documents like tweets, comments or sentences. See Liu \shortcite{Bing12}, chapters three to five and references therein for the description of several opinion classification methods. 

In this paper we focus on classifying the opinion toward religious/political topics, say the Quran, in political discussion by using the lexical-based approach. 
One intuitive approach is to find both the keyword (e.g. Quran) and the words with sentiment in the sentence and classify the sentiment of the sentence based on the polarity of these sentiment words. We expect that statistically the importance of a sentiment word towards the keyword is related on the number of words between the sentiment and key word as suggested by Ding et al. \shortcite{Ding08}. Two other approaches is to automatically parse the material and either use the distance between key and sentiment word in the parse tree or develop grammatical dependence paths, see e.g. Jiang et al. \shortcite{Jiang11}. The aim of this paper is to compare the performance of a word distance method \cite{Ding08} with a developed method based on distance in parse tree and grammatical dependence paths to classify opinions in political discussions.

The paper is organized as follows. 

\section{Opinion mining methods}
\label{sec:om}

In this Section we present two methods to classify sentences to either positive, neutral or negative towards a keyword. Both methods follow the same general algorithm presented below which is inspired by Ding et al. \shortcite{Ding08} and is based on a sentiment lexicon. Both keywords, sentiment words and sentiment shifters can in general appear several times in a sentence. Sentiment shifters is words that potentially shift the sentiment of a sentence from positive to negative or negative to positive. E.g. ``not happy'' have the opposite polarity than just ``happy''. We assume that we have a list of sentiment words each associated with a sentiment score representing the polarity and strength of the sentiment word (sentiment lexicon). Let $kw_i, i \in \{1,2,\ldots,I\}$ represent appearance number $i$ of the keyword in the sentence. Further let $sw_{j}, j \in \{1,2,\ldots,J\}$ be appearance number $j$ of a sentiment word in the sentence. Finally let $\textbf{ss} = (ss_1, ss_2, \ldots, ss_K)$ repressent the sentiment shifters in the sentence. We compute a sentiment score, $S$, for the sentence as follows
\begin{equation}
  \label{eq:1}
  S = \frac{1}{I}\sum_{i=1}^{I} \sum_{j=1}^{J} \mathbf{imp}(kw_i, sw_{j})\mathbf{shift}(sw_{j}, \mathbf{ss})    
\end{equation}
where the function Imp computes the importance of the sentiment word $sw_{j}$ on the keyword appearance $kw_i$. This will be computed in different ways as described below. Further, the function $\mathbf{shift}(sw_{j}, \mathbf{ss})$ computes whether the sentiment of $sw_{j}$ should be shifted based on all the sentiment shifters in the sentence. It returns $-1$ (sentiment shift) if some of the sentiment shifters is within $d_{p}$ words in front or $d_{n}$ words behind $sw_{j}$, respectively. Else the function, returns $1$ (no sentiment shift). We classify the opinion towards the keyword to be positive, neutral or negative if $S >= t_p$,  $t_p > S > t_n$ and  $S <= t_n$, respectively. The parameters $d_p, d_n, t_p$ and $t_n$ is tuned using a training set.

\subsection{Word distance method}
\label{sec:wd}

For the word distance method we use the following Imp function
\begin{equation}
  \label{eq:2}
  \mathbf{imp}(kw_i, sw_{j}) = \frac{\mathbf{sentsc}(sw_{j})}{\mathbf{worddist}(kw_i, sw_{j})}
\end{equation}
where $\mathbf{sentsc}(sw_{j})$ is the sentiment score of $sw_{j}$ from the sentiment lexicon and $\mathbf{worddist}(kw_i, sw_{j})$ is the number of words between $kw_i$ and $sw_{j}$ in the sentence plus one.

\subsection{Parse tree method}
\label{sec:dp}

LEGGE INN FRA LILJA OG PER ERIK OM PARSING OG DEPENDENCE PATHS.

Let $\mathcal{D}$ denote the set of all important grammatical relations. The function $\mathbf{gram}(kw_i, sw_{j})$ returns the grammatical relation, and if $\mathbf{gram}(kw_i, sw_{j}) \in \mathcal{D}$, then the function $W_{\mathbf{dep}}(kw_i, sw_{j}) \in [0,1]$, return the importance of the grammatical relation. Further let $\mathbf{treedist}(kw_i, sw_{j})$ return the number of words between the two words in the parse tree plus one. The Imp function is computed as follows. If $\mathbf{gram}(kw_i, sw_{j}) \in \mathcal{D}$ we use
\begin{align}
  \begin{split}
    \label{eq:3}
  \mathbf{imp}&(kw_i, sw_{j}) = \\
  &\alpha \cdot \mathbf{sentsc}(sw_{j}) W_{\mathbf{dep}}(kw_i, sw_{j}) \\
  +&(1 - \alpha) \cdot \dfrac{\mathbf{sentsc}(sw_{j})}{\mathbf{treedist}(kw_i, sw_{j})}    
  \end{split}
\end{align}
where $\alpha \in [0,1]$ is parameter that weights the score from the important dependence path and the tree distance may be tuned from a training set. If $\mathbf{gram}(kw_i, sw_{j}) \not\in \mathcal{D}$ we simply use 
\begin{equation}
  \label{eq:4}
  \mathbf{imp}(kw_i, sw_{j}) = \dfrac{\mathbf{sentsc}(sw_{j})}{\mathbf{treedist}(kw_i, sw_{j})}    
\end{equation}
Note that when $\alpha = 0$, \eqref{eq:3} reduces to \eqref{eq:4}.

\subsection{Statistical analysis of classification performance}
\label{sec:sa}

We compare the classification performance of a set of $M$ different methods, denoted as $\Pi_1, \Pi_2, \ldots, \Pi_M$, using a random effect logistic regression. Let the stochastic variable $Y_{tm} \in \{0,1\}$ represents whether method $\Pi_m, \, m \in \{1,2,\ldots,M\}$ classified the correct opinion to sentence number $t \in \{1,2,\ldots,T\}$, where $T$ is the number of sentences in the test set. We let $Y_{tm}$ being the dependent variable of the regression model. The different methods $\Pi_1, \Pi_2, \ldots, \Pi_M$ is included as a categorical independent variable. We also assume that classification performance of the different methods depends on the sentence to be classified, thus the sentence number therefore is included as a random effect. The model is then formulated as
\begin{align}
  \label{eq:5}
  \begin{split}
    P(Y_{tm} = 1) &= \text{logit}(\mu + \Pi_m + \epsilon_t + \epsilon_{tm}) \\
    P(Y_{tm} = 0) &= 1 - P(Y_{tm} = 1)
  \end{split}
 \end{align}
where $\epsilon_t$ is the sentence number random effect and $\epsilon_{tm}$ is additional random noise. Fitting the model to the observed classification performance of the different methods we are able to see if the probability of classifying correctly significantly vary between the methods.

The statistical analysis is performed using the statistical program R \cite{R} and the R package \verb|lme4| \cite{lme4}.


\section{Real data example}
\label{sec:results}

\subsection{Text material}

We did not find any suitable annotated text material related to political discussions and therefore created our own. We manually selected 46 debate articles from the Norwegian online newspapers \textit{NRK Ytring, Dagbladet, Aftenposten, VG and Bergens Tidene}. To each debate article there were attached an discussion thread where readers could express their opinions and feelings towards the content of the debate article. All the text from the debate articles and the subsequent discussions were collected using text scraping \cite{Hammer13}. The debate articles were related to religion and immigration and we wanted to classify the opinion towards all words with stem: \textit{islam, muslim, quran, allah, muhammed, imam and mosque}. These are topics that often creates a lot of active discussions and disagreements.

We automatically divided the material into sentences and all sentences containing at least one keyword and one sentiment word were kept for further analysis. If a sentence contained more than one keyword, e.g. both islam and quran, the sentence were repeated one time for each keyword. We could then classify the opinion towards each of the keywords in the sentence consecutively. To assure that we do not underestimate the uncertainty in the statistical analysis, we see each repetition of the sentence as the same sentence with respect to the sentence random effect in the regression model in Section \ref{sec:sa}

Each sentence were manually annotated whether the commenter were positive, negative or neutral towards the keyword in the sentence. Each sentence were evaluated individually. The sentences were annotated based on all our knowledge, e.g. a sentence like ``Muhammed is like Hitler'' would be annotated as a negative opinion towards Muhammed. Further, if a commenter presented a negative fact about the keyword, the sentence would be denoted as negative.\\
INTERANNOTARY!!\\
Finally the material were divided in to two parts where the first half of the debate articles with subsequent discussions where in the training set and the rest were in the test set. The researcher working on finding the important dependence paths, did only use the training set and did never see the test set before the decisions about dependence paths were decided. After the division, the training and test set consisted of a total of 382 and 308 sentences, respectively. Table \ref{tab:1} summarizes the opinions in the sentences
\begin{table}
  \caption{Manual annotation of training and test set.}
  \centering
  \begin{tabular}{lcccc}
             & Negative & Neutral & Positive \\\hline
    Training & 174 (46\%) & 162 (42\%) & 46 (12\%)\\
    Test     & 102 (33\%)& 182 (59\%) & 24 (8\%)
  \end{tabular}
  \label{tab:1}
\end{table}

\subsection{Sentiment lexicon and sentiment shifters}

Unfortunately, no sentiment list exists for the Norwegian language and therefore we developed our own by manually translating the AFINN list \cite{Nielsen11}. We also manually added 1590 words relevant to political discussions  like 'deport', 'expel', 'extremist', 'terrorist' and so on, ending up with a list of 4067 Norwegian sentiment words. Each word were given a score from $-5$ to $5$ ranging from words with extremely negative sentiment (e.g. 'behead') to highly positive sentiment words (e.g. 'breathtaking').

Several Norwegian sentiment shifters were considered but only the basic shifter 'not' improved the opinion classification and therefore only this word were used in the method.

\subsection{Classification performance}

In this study we compared four different methods based on the general algorithm in \eqref{eq:1}.
\begin{itemize}
\item We use the \textbf{imp}-function presented in \eqref{eq:2}. We denote this method WD (word distance).
\item For this method and the two below we use the \textbf{imp}-function in \eqref{eq:3}. Further we set $\alpha = 0$ meaning that we do not use the important dependence paths. We denote this method A0 ($\alpha = 0$).
\item We set $\alpha = 1$ and for all dependency paths we set $W_{\mathbf{dep}} = 2/3$. We denote this method CW (constant weights).
\item We set $\alpha = 1$ and for $W_{\mathbf{dep}}$ we use the weights presented in Table ??????. We denote this method OD (optimal use of dependence paths)
\end{itemize}
For each method we used the training set to manually tune the parameters $d_p, d_n, t_p$ and $t_n$ of the method. The parameters were tuned to optimize the number of correct classifications.

Table \ref{tab:2} shows the optimal parameter values of $d_p, d_n, t_p$ and $t_n$ and the classification performance for different methods. 
\begin{table}
  \caption{The second to the fifth column show the optimal values of the parameters of the model. The sixth column show the number of correct classifications and the last column shows the p-value that the method performs better than WD.}
  \centering
  \begin{tabular}{lcccccl}
       & $d_p$ & $d_n$ & $t_p$ & $t_n$ & Correct   & p-val \\ \hline
    WD &    2  &   0  & 0.7  &  0.0  & 145 (47\%) & \\
    A0 &    2  &   0  & 2.0  &  0.3  & 161 (52\%) & 0.047 *\\
    CW &    2  &   0  & 2.0  &  0.3  & 161 (52\%) & 0.048 *\\
    OD &    2  &   0  & 2.0  &  0.3  & 162 (53\%) & 0.031 *
  \end{tabular}
  \label{tab:2}
\end{table}
We see that the sentiment shifter 'not' only have a positive effect on classification performance when it is in front of the sentiment word. We see that using dependence tree distances (method A0) the classification results is significantly improved compared to using word distances in the sentence (method WD) (p-value = 0.047). Also including the important dependence paths (method OD) further improves the classification performance (p-value = 0.031).

\section{Closing remarks}
\label{sec:cr}

Classifying opinions in political discussions is hard because of the frequent use of irony, sarcasm and citations. 

In this paper we have compared the use of word distance between keyword and sentiment word against metrics related to parsed sentence information. 

\bibliography{bibl}

\end{document}
